pdf_path: "path/to/your/document.pdf" # actually supports all file types LlamaIndex supports, see: https://github.com/run-llama/llama_index/blob/6c72f27ab52a8214a4a496aaf876a296061fad57/llama-index-core/llama_index/core/readers/file/base.py#L65
num_topics: 5
num_turns: 6
output_dir: "output"
export_conversation_json: true

models:
# Your model names MUST be in "<provider-name>/<model-name>" format
  topic_generator: "ollama/qwen2.5:14b"
  podcast_moderator: "ollama/llama3.1:8b"
  podcast_host: "ollama/llama3.1:8b"
  podcast_guest: "ollama/llama3.1:8b"
  intro_generator: "ollama/qwen2.5:14b"
  outro_generator: "ollama/llama3.1:8b"
  translator: "ollama/qwen2.5:14b"
  providers:
    openai_api_key: "sk-..."       # Your OpenAI API key
    anthropic_api_key: "sk-ant-..." # Your Anthropic API key
    groq_api_key: "gsk_..."         # Your Groq API key
    ollama_base_url: "http://localhost:11434"  # Default Ollama server URL

audio:
  lang: "e"  # Language codes: see Language Codes section below
  host_voice: "ef_dora"       # Voice name (see Available Voices section)
  moderator_voice: "em_alex" # Voice name
  guest_voice: "em_santa"    # Voice name
  output_file: "podcast.wav"
  chunk_size: 200
  music_path: "path/to/background/music"
  vocal_volume: 0  # in dB
  bg_intro_volume: -12  # optional
  bg_content_volume: -20  # optional
  bg_outro_volume: -12
  generate_subtitles: true   # Generate subtitle files
  subtitle_format: "srt"     # "srt" or "vtt"

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/podcast_generator.log"
  verbose: true  # Show colored conversation in console during generation
